% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/ScrapeCraigslist.R
\name{ScrapeCraigslist}
\alias{ScrapeCraigslist}
\title{Craigslist Scraper}
\usage{
ScrapeCraigslist(url, row.selector, fields)
}
\arguments{
\item{url}{A text string of the link to crawl}

\item{row.selector}{The CSS selector that demarcates the rows on a page.}

\item{fields}{A text vector of the CSS fields used to identify desired
information within the rows.}
}
\value{
A single data frame containing all the elements scraped, with the CSS selector
listed as the name of each column.
}
\description{
From a general list style page on Craigslist, scrape the url with desired
data using css selectors.
}
\details{
For Craigslist pages, this function finds the row on a page,
and using CSS selector that you specify, scrapes data from each row into a
nice data frame for research and munging purposes. This function extensively
uses \code{rvest} functions.

To get the CSS selectors, I used the SelectorGadget chrome extension. For
more info, see \href{http://selectorgadget.com}{selectorgadget.com}.
}
\examples{
url <- 'http://philadelphia.craigslist.org/search/apa'
fields <- c('time', '.hdrlnk', '.housing', 'small', '.price')
philly.apts <- ScrapeCraigslist(url, '.row', fields)

# Clean up the price data
philly.apts[, 5] <- as.integer(gsub('\\\\$', '', philly.apts[, 5]))
}

